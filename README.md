# ğŸ½ï¸ Food Recognition Models with PyTorch

Welcome to the FoodNet project! This repository contains state-of-the-art food recognition models built using PyTorch and trained on the Food101 dataset. Easily classify food images into 101 delicious categories with modern deep learning techniques.

---

## ğŸš€ Features
- **PyTorch-based**: Flexible, modular, and easy-to-extend codebase.
- **Food101 Dataset**: Trained and evaluated on the popular Food101 dataset.
- **Data Augmentation**: Advanced image augmentations for robust models.
- **Custom Architectures**: Includes TinyVGG and more.
- **Jupyter Notebooks**: Interactive experiments and visualizations.

---

## ğŸ“‚ Project Structure
```
FoodNet/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ notebooks/         # Jupyter notebooks for experiments
â”‚   â”œâ”€â”€ models/            # Model definitions
â”‚   â””â”€â”€ helper/            # Utility scripts
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ setup.py               # Project setup
â””â”€â”€ README.md              # Project documentation
```

---

## ğŸ¥— Dataset
- **Food101**: 101 food categories, 101,000 images.
- Downloaded and managed automatically via torchvision.

---

## ğŸ› ï¸ Installation
1. Clone the repository:
   ```bash
   git clone https://github.com/yourusername/FoodNet.git
   cd FoodNet
   ```
2. Create and activate a virtual environment (recommended):
   ```bash
   python -m venv mlvenv
   # On Windows:
   .\mlvenv\Scripts\activate
   ```
3. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

---

## ğŸ“Š Usage
- Run and explore the Jupyter notebooks in `src/notebooks/` to train, evaluate, and visualize models.
- Example: `tinyvgg.ipynb` demonstrates training a TinyVGG model on Food101.

---

## ğŸ§‘â€ğŸ’» Author
- **Amal Varghese**  
  [officialamalv2004@gmail.com](mailto:officialamalv2004@gmail.com)

---

## â­ï¸ Contributing
Pull requests, issues, and suggestions are welcome! Feel free to fork the repo and submit improvements.

---

## ğŸ“œ License
This project is licensed under the MIT License.

---

